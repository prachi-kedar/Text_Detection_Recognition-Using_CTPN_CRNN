{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "#'''\n",
    "# Created on 18-12-11 上午10:01\n",
    "#\n",
    "# @Author: Greg Gao(laygin)\n",
    "#'''\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class RPN_REGR_Loss(nn.Module):\n",
    "    def __init__(self, device, sigma=9.0):\n",
    "        super(RPN_REGR_Loss, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        '''\n",
    "        smooth L1 loss\n",
    "        :param input:y_preds\n",
    "        :param target: y_true\n",
    "        :return:\n",
    "        '''\n",
    "        try:\n",
    "            cls = target[0, :, 0]\n",
    "            regr = target[0, :, 1:3]\n",
    "            regr_keep = (cls == 1).nonzero()[:, 0]\n",
    "            regr_true = regr[regr_keep]\n",
    "            regr_pred = input[0][regr_keep]\n",
    "            diff = torch.abs(regr_true - regr_pred)\n",
    "            less_one = (diff<1.0/self.sigma).float()\n",
    "            loss = less_one * 0.5 * diff ** 2 * self.sigma + torch.abs(1- less_one) * (diff - 0.5/self.sigma)\n",
    "            loss = torch.sum(loss, 1)\n",
    "            loss = torch.mean(loss) if loss.numel() > 0 else torch.tensor(0.0)\n",
    "        except Exception as e:\n",
    "            print('RPN_REGR_Loss Exception:', e)\n",
    "            # print(input, target)\n",
    "            loss = torch.tensor(0.0)\n",
    "\n",
    "        return loss.to(self.device)\n",
    "\n",
    "\n",
    "class RPN_CLS_Loss(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(RPN_CLS_Loss, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        y_true = target[0][0]\n",
    "        cls_keep = (y_true != -1).nonzero()[:, 0]\n",
    "        cls_true = y_true[cls_keep].long()\n",
    "        cls_pred = input[0][cls_keep]\n",
    "        loss = F.nll_loss(F.log_softmax(cls_pred, dim=-1), cls_true)  # original is sparse_softmax_cross_entropy_with_logits\n",
    "        # loss = nn.BCEWithLogitsLoss()(cls_pred[:,0], cls_true.float())  # 18-12-8\n",
    "        loss = torch.clamp(torch.mean(loss), 0, 10) if loss.numel() > 0 else torch.tensor(0.0)\n",
    "        return loss.to(self.device)\n",
    "\n",
    "\n",
    "class basic_conv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_planes,\n",
    "                 out_planes,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 relu=True,\n",
    "                 bn=True,\n",
    "                 bias=True):\n",
    "        super(basic_conv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CTPN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base_model = models.vgg16(pretrained=False)\n",
    "        layers = list(base_model.features)[:-1]\n",
    "        self.base_layers = nn.Sequential(*layers)  # block5_conv3 output\n",
    "        self.rpn = basic_conv(512, 512, 3, 1, 1, bn=False)\n",
    "        self.brnn = nn.GRU(512,128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_fc = basic_conv(256, 512, 1, 1, relu=True, bn=False)\n",
    "        self.rpn_class = basic_conv(512, 10 * 2, 1, 1, relu=False, bn=False)\n",
    "        self.rpn_regress = basic_conv(512, 10 * 2, 1, 1, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_layers(x)\n",
    "        # rpn\n",
    "        x = self.rpn(x)    #[b, c, h, w]\n",
    "\n",
    "        x1 = x.permute(0,2,3,1).contiguous()  # channels last   [b, h, w, c]\n",
    "        b = x1.size()  # b, h, w, c\n",
    "        x1 = x1.view(b[0]*b[1], b[2], b[3])\n",
    "\n",
    "        x2, _ = self.brnn(x1)\n",
    "\n",
    "        xsz = x.size()\n",
    "        x3 = x2.view(xsz[0], xsz[2], xsz[3], 256)  # torch.Size([4, 20, 20, 256])\n",
    "\n",
    "        x3 = x3.permute(0,3,1,2).contiguous()  # channels first [b, c, h, w]\n",
    "        x3 = self.lstm_fc(x3)\n",
    "        x = x3\n",
    "\n",
    "        cls = self.rpn_class(x)\n",
    "        regr = self.rpn_regress(x)\n",
    "\n",
    "        cls = cls.permute(0,2,3,1).contiguous()\n",
    "        regr = regr.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        cls = cls.view(cls.size(0), cls.size(1)*cls.size(2)*10, 2)\n",
    "        regr = regr.view(regr.size(0), regr.size(1)*regr.size(2)*10, 2)\n",
    "\n",
    "        return cls, regr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
